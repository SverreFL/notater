{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datastrukturer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bygger på numpy arrays og arver mye funksjonalitet derfra. Det er to sentrale objekt: Series og DataFrame\n",
    "\n",
    "I tillegg til den implisitte numeriske indexen til arrays har den også et eksplisitt indexobjekt som mapper til element i Series eller til Series i DataFrame. Koblingen mellom indeks og verdi gjør at vi kan kombinere data fra ulike kilder, håndtere missing data og generelt ikke er avhengig av å ha helt konforme data (samme størrelse, samme rekkefølge) for å gjøre regneoperasjoner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series er crosseover mellom array og dict. Består av to arrays: én index og én med homogene data. Vanlige ndarrays har implisitt index med posisjon, men her er det eksplisitt og kan ha ulike labels. Kan også tenke på Series som en dict som mapper index-verdier til verdi i array. Følgelig er det en del metoder/syntax som tilsvarer dict. Kan få ut hver av arraysene med:\n",
    "1. a.values (ndarray)\n",
    "2. a.index (index-objekt, finnes litt ulike typer, har diverse metoder, noe mengdegreier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame er tabulær datastruktur med både rekke-index og kolonne-index. I utgangspunktet er det ganske symmetrisk, men av konvensjon angir rekke-index obseravjon og kolonne-index er variabel (dimensjon/egenskap) ved observasjonen. Kan betrakte det som en dict som mapper label til Series, der labels er enten fra rekke- eller kolonneindex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det er en fullverdi datastruktur/objekt i seg selv. Har en del metoder og sånn. Vet ikke om jeg må jobbe så mye direkte med indexen; kan gjøre operasjon på dataframe og så håndterer de index internt.\n",
    "\n",
    "\n",
    "Merk at når vi setter en ny kolonne som indeks mister vi den eksisterende index-kolonnnen. Hvis vi vil beholde må vi lagre kopi av kolonne og deretter putte den inn i nye dataframe. Kan få tak i kolnnen ved å bruke df.index() og deretter dytte den inn i df ved å assigne kolonnen med df[\"navn\"] = df.index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indeksing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan legge til verdier på subset av dataframe\n",
    "\n",
    "```python\n",
    "df.loc['a'] = arr\n",
    "df.loc['a'] = df_sub # fungerer ikke selvom matchene index, vet ikke hvorfor ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kolonner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan merke at kolonnene egentlig bare er index og at det i utgangspuntket er ganske symmetrisk.\n",
    "\n",
    "Vil ha deskriptive kolonnenavn. Bruker\n",
    "- df.rename(columns=mapper), der mapper er dictionary med {'gammel':'ny',:}\n",
    "\n",
    "Tror min konvensjon er å ha navn i snake_case, kan bruke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan bruke vanlig list comprehension til å finne subset av kolonner som oppfyller kriterie,\n",
    "\n",
    "```python\n",
    "cols_subset = [col for col in df.columns if 'string' in col]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index med flere level. Ytterste er level=0, og så teller vi oppover "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Konstruere multiindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan eksplisitt konstruere multiindex objekt på flere måter\n",
    "\n",
    "```python\n",
    "arrays = [np.array([\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"]),\n",
    "          np.array([\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"])]\n",
    "\n",
    "# Tre ekvivalente indekser:\n",
    "pd.MultiIndex.from_tuples(zip(arrays[0],arrays[1])) # trenger list of tuples\n",
    "pd.MultiIndex.from_arrays(arrays) # trenger list of arrays\n",
    "pd.MultiIndex.from_frame(pd.DataFrame(arrays).T) # organisere arrays i dataframe før vi lage index\n",
    "\n",
    "pd.MultiIndex.from_product(iterables) # alle unike kombinasjoner av verdier i ulike arrays\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan også konstruere MultiIndex i constructor for DataFrame, må da være list of arrays\n",
    "```python\n",
    "df = pd.DataFrame(index=arrays, data=np.random.randn(len(index)), columns=['tall'])\n",
    "df.set_index('tall', append=True) # legge til kolonne på eksisterende index\n",
    "\n",
    "# Hvis vi ikke har eksisterne index kan vi lage nye multiindex med:\n",
    "df.set_index([\"kol1\",\"kol2\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis vi har lyst til å legge ny array (eller eventuelt konstant verdi) som øvereste level i multiindex bør vi transformere index til dataframe så den  blir enklere å manipulere\n",
    "```python\n",
    "temp = df.index.to_frame()\n",
    "temp.insert(0, 'name', value)\n",
    "df.index = pd.MultiIndex.from_frame(temp)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slice multiindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsetting av observasjoner avhengig av verdier i multiindex kan bli ganske komplisert.\n",
    "\n",
    "```python\n",
    "\n",
    "df.loc['a'] # alle rekker der index i level 0 == 'a'\n",
    "df.loc[('a', 'b')] # alle rekker der level 0 == 'a' og level 1 == 'b'\n",
    "```\n",
    "\n",
    "Må se mer på dette senere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sortering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan sortere entent etter index eller verdi langs gitt kolonne,\n",
    "1. df.sort_index()\n",
    "2. df.sort_values(['index','kol']), kan sende inn både navn på index og kolonne..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lage dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fra fil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spesifiserer hvordan pd.readcsv() skal laste inn data\n",
    "\n",
    "1. usecols = [..] for å spesifisere subset av kolonner vi vil laste inn\n",
    "2. names = [..] hvis kolonnene ikke har navn fra før så kan vi spesifisere de.\n",
    "3. Bestemme hvilken kolonne i datasett som skal fungere som index, index_col = <tall>. \n",
    "4. Dersom det er rekke vi ikke vil laste inn, skiprows = <tall>, skipper de n øverste.\n",
    "5. parse_dates = True ---> finner kolonne med date, gjør den til index, får den i datetime.\n",
    "    \n",
    "Kan også laste inn fra andre filtyper enn .csv, for eksempel .json. Tar da orient keyword med verdi \"split\", \"records\" eller \"columns\"\n",
    "    \n",
    "pd.read_json(orient=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pd.read_csv(filename,\n",
    "            sep=',', # kan bruke andre seps, tror tab er '\\t'\n",
    "            delimiter=None,\n",
    "            header='infer', # må spesifisere header=0 for å endre navn..\n",
    "            names=None,\n",
    "            index_col=None, # hvis første kol skal være index bruker vi =0\n",
    "            usecols=None,\n",
    "            parse_dates, # ?\n",
    "            \n",
    "            ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fra datastrukturer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan ha lyst til å lage tabell som mapper index til verdi. F.eks. tabell med som mapper variabelnavn til reg koef. Har to lister. Et alternativ er pd.Series(data=x,index=y). Annet alternativ er å først konstruere dict:\n",
    "\n",
    "pd.Series(dict(zip(y,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagre dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan lagre dataframes som består av tall og strenger som tekst i csv format med `to_csv`\n",
    "\n",
    "Hvis det inneholder andre objekter, så må vi lagre til `to_pickle`. Generelt vil pickle lagre fullstendig informasjon om dataframe; alt fra datatyper, form av indeks, ... alt. Når vi lagrer til csv så er det bare en lang string og vi må tolke denne stringen når vi laster dataframe, så da starter vi gjerne fra scratch. Pickle er bedre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beskrive data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Før jeg kan gjøre noe analyse må jeg først undersøke tabellen med tall jeg har fått utdelt. Må få oversikt over:\n",
    "1. Hvilke egensakper (kolonner) og hva de måler\n",
    "2. Hva slags måleenhet de er i og konvertere til riktig datatype. I algebraen under the hood er alt tall, men representer som factor for å få bedre representasjon og utnytte funksjonalitet i plotte-libraries og lignende. Vil unngå å jobbe eksplisitt med dummies før det er nødvendig.\n",
    "3. Få noe oversikt over univariate fordelinger og eventuelt parvis bivariate korrelasjoner. Se at tall er rimelig, blir kjent med data, oppdage numerisk kodete missing values, se etter feilkodinger\n",
    "4. Få oversikt over manglende verdier, vurdere strategi for imputation og eventuelt bias dersom vi ser på delutvalg med fullstendig obervasjon.\n",
    "\n",
    "Kan bruke .describe() --- output avhenger av datatype\n",
    "\n",
    "Kan bruke .info() til å få datatype og observere om det er missing values\n",
    "\n",
    "Kan bruke .ndtype() for å finne datatypene\n",
    "\n",
    "Hvis vi har en series kan vi bruke .value_count() for å få antallet observasjoner med gitt verdi langs den dimensjonen.\n",
    "\n",
    "Kan bruke .unique() for å få liste av unike verdier og .unique() for antallet unike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datatyper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hva slags type data vi har påvirker gyldige operasjoner og analyse av data. Derfor viktig å kategorisere. Hver kolonne er homogen.\n",
    "\n",
    "1. Ulike numeriske datatyper (int,float,..)\n",
    "2. category\n",
    "3. object\n",
    "\n",
    "Kan lage ordered categorical med pd.Categorical(df.col, ordered=True, categories = [..]), der categories er optional og kun dersom jeg trenger gitt rekkefølge (fra lav til høy).\n",
    "\n",
    "Kan endre datatype til flere kolonner ved : df[liste_av_kolonner] = df[liste_av_kolonner].astype('dtype') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "cat_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "df[cat_cols] = df[cat_cols].astype('category')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan lage kategorier fra numerisk verdi med dictionary som mapper verdi til kategori og col.map(table). Det forutsetter at mapping er exhaustive siden resten blir nans. Kan eventuelt bruke .fillna(df['col1']) etterpå for å beholde andre verdier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beskrive kolonner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruker value_counts(normalize=).sort_index() # normalize hvis jeg vil ha som prosent\n",
    "\n",
    "gir oversikt over antall unike verdier, hvor tyngden av data ligger og om det er verdier som ikke gir mening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vil representere med NaN. Kan lage med np.nan. Dette er en float, slik at kolonner med missing values blir recast til floats. Det er litt teknisk hvordan det blir representert under the hood, får eventuelt se på det senere. I R er det na og null, i pandas bare NaN (finnes en mer moderne datatype na som de har lagt til).\n",
    "\n",
    "Missing values propapegerer når vi gjør operasjoner på dataframe. Operasjon er elementvis og alt som bruker NaN resultererr i NaN. Dette gjelder også får aggregeringsfunksjoner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Første jeg må gjøre er å sjekke antallet missing values i ulike kolonner. Husk at missing values kan være kodet på ulike måter i ulike datasett. En mulighet er å bruke df['col'].unique() til å se på verdier og se om noen ikke gir mening. Deretter vil jeg omkode de til NaN, f.eks. ved df.replace(value,np.nan)\n",
    "\n",
    "- df.isna() gir boolean mask over hele dataframe\n",
    "- df.isna().any() angir om hvorvidt det er nans i hver av kolonner\n",
    "- df.isna().sum() gir antallet nans i hver kolonne\n",
    "\n",
    "Når jeg har fått oversikt kan jeg vurdere å filtrere de ut. Bruker da df.dropna() med opsjoner\n",
    "- axis= .. droppe kolonne eller rekke avhengig om jeg finner nans der\n",
    "- how='any','all'\n",
    "- tresh= antall nans før jeg dropper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En alternativ fremgangsmåte er å fylle inn verdier. Algoritmer i sklearn kan ikke håndtere nans og det er synd å kaste bort data bare fordi noen observasjoner er litt mangefulle. Finnes ulike strategier for dette som jeg kan se på senere\n",
    "1. Bruke gjennomsnitt i kolonnen\n",
    "2. Predikere verdi ut fra andre dimensjoner vi observere\n",
    "3. Bruke verdi fra naboer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays i dataframe er homogone. Dersom det er noen symboler i kolonne som i utgangspunktet skal være numerisk vil pandas tolke det som string. Må fjerne symbolene før vi kan konvertere.\n",
    "```python\n",
    "def to_numeric(s):\n",
    "    return int(''.join(ch for ch in s if ch.isnumeric()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split-apply-combine workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi har et stort datasett. Det er mange variabler og mange kolonner. Hvordan skal vi få ut noen innsikter fra all denne informasjonen?\n",
    "\n",
    "Kan vært veldig nyttig å dele det inn i mindre deler etter felles verdi langs kolonne. Kan tenke at det er observasjoner som \"hører sammen\". Deretter annvender vi noen aggregeringsfunksjoner som gir oss slags sammendragsmål for tallverdiene i hver del. Deretter kan vi kombinere det sammen i nytt datasett som mapper nøkkel (felles verdi) til verdi av aggregeringsfunksjon.\n",
    "\n",
    "Okay, la oss se på dette i praksis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gbA = df.groupby('A') gir oss et groupby objekt. Det er poeng at hver gruppe er en dataframe. Kan få ut disse dataframene med gbA.get_group(verdi av A). Kan gruppere med verdier langs flere kolonner; dette gir og PxK grupper, der P er verdi langs første og K er verdi langs andre. I den kombinere dataframen vil har multiindex. Kan også observere at hver observasjon er i én gruppe.\n",
    "\n",
    "Kan anvende aggregeringsmetoder direkte på gbA. Kan også definere custom aggregeringfunksjon og anvende de med .agg metoden\n",
    "\n",
    "gbA.agg(lambda x: (x > 0).count()) gir antaller positive observasjon. Kan være greit å gi funksjonene navn så blir litt mer ryddig.\n",
    "\n",
    "For å øke fleksibiliteten til vår oppdeling av datasett kan vi lage et Grouper objekt. Stor fordel her Grouperen vår kan tolke datetime objekter. Hvis key=series med datetime så kan vi slenge in freq=\"W\" og gruppere observasjoner for hver uke\n",
    "\n",
    "eksempel: gb = df.groupby(pd.Grouper(key='Date', freq ='W')) der Date er kolonne med datetime objekt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det er en måte å konstruere separate dataframes filtrert på verdi av kolonne vi grupper etter. Kan få tak i de interne dataframene med\n",
    "```python\n",
    "for idx, df in df.groupby('col')[andre_cols]:\n",
    "    print(idx) # verdi av col\n",
    "    print(df) # dataframe filtrert på col == idx\n",
    "```    \n",
    "i praksis så bruker vi groupby ofte til å lage en ny dataframe med sammendragsmål på betinget fordeling for hver verdi av 'col'.\n",
    "\n",
    "er noe distinksjon mellom dataframe groupby og series groupby. får dataframe groupby hvis jeg spesifiser as_index=False. hmm. poeng at det kjører metode på entent dataframe eller series under the hood og er ikke fullstendig overlapp der...\n",
    "\n",
    "vil i alle tilfeller at output er dataframe i stedet for series. kan oppnå dette med .reset_index(). Viktig at series har navn da slik at jeg får navn i kolonne av ny dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis det er jeg grupperer etter id på stasjon og det er samme navn på alle stasjon med samme id.. og jeg vil ha ut kolonne med id og navn. Så kan jeg bruke .first() i stedet for å aggregere\n",
    "\n",
    "```python\n",
    "cols = ['start_station_name', 'start_station_latitude', 'start_station_longitude']\n",
    "stations = df.groupby('start_station_id')[cols].first()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-level groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan ha lyst til å gruppere på to eller flere kolonner samtidig. En mulig fremgangsmåte ville vært å ta groupby på hver group av første kolonne og så fått alle verdiene inn en ny dataframe, eg\n",
    "```python\n",
    "df_out = pd.DataFrame(index=some_index)\n",
    "gb = df.groupby('first_column')\n",
    "for idx, group in gb:\n",
    "    group.loc[idx] = group.groupby('second_column')['some_column'].mean()\n",
    "```\n",
    "eller noe sånt. Dette er veldig rotete. Heldigvis gjør pandas dette enkelt ved å sende inn flere kolonner i groupby\n",
    "```python\n",
    "df_out = df.groupby(['first_column', 'second_column'])['some_column'].mean()\n",
    "```\n",
    "Merk at df_out bare vil ha index for de kombinasjoner av verdier i kolonnene vi grupperer på som vi faktisk observerer i data. I mange tilfeller kan det være greit å ha alle kombinasjonene og padde med NaNs for de vi ikke observerer. Litt usikker på hvordan jeg skal gjøre dette... må resette/konstruere denne indeksen på en måte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brukes for å aggrere tidsserie observasjoner innenfor tidsintervall.\n",
    "\n",
    "```python\n",
    "df.resample(rule='H', # hourly, daily, weekly, monthly... spesifisere intervall\n",
    "            on='started_at' # kolonne med tidsdimensjon (hvis ikke indeks)\n",
    "            )\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grouper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruker inni groupby for mer kontroll.\n",
    "\n",
    "Eksempel: gruppere observasjoner med timedelta innefor 5 minutters intervall\n",
    "```python\n",
    "duration_agg = df.groupby(pd.Grouper(key='duration',freq='5Min'))['started_at'].agg('count')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .agg() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan bruke hvis vi vil kjøre ulike aggegreringsfunksjon på ulike kolonner\n",
    "```python\n",
    "df_out = (df.groupby(\"key\").\n",
    "              agg({\"col1\":'mean', 'col2':'count'}).\n",
    "              rename(columns={'col1':'avg_col1', 'col2':'num_col2'})\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lage en df fra eksisterende df for å analysere spesifikk problem. Bestemmer index. Lar verdiene i én av kolonnene være kolonner(kategorier) i ny df. Bestemmer hvilken kategori som skal være verdi for hver index langs nye kolonner. Spesifisere aggregeringsfunksjon for alle verdiene for gitt katagori for hver index (flere observasjoner inn i samme rute --> aggregering til ett tall).\n",
    "\n",
    "eks: df.pivot_table(values=BNP , index=ÅR ,columns =LAND, agg=np.mean) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Vi kan representerer de samme dataene på ulike måter. \n",
    "\n",
    "1. Long (alt er i index...)\n",
    "2. Wide (alt er i kolonne...)\n",
    "\n",
    "Vi har fire grunnoperasjon for å omforme data\n",
    "\n",
    "1. stack (sender ting ned i index, flere levels på indexen)\n",
    "2. unstack (sender ting opp i kolonne, mindre levels på indexen)\n",
    "3. set_index\n",
    "4. reset_index\n",
    "\n",
    "Har operasjoner som bruker disse under the hood\n",
    "\n",
    "1. melt (gjør long)\n",
    "2. pivot (index, col, value) <- wide?\n",
    "3. pivot_table, generalisering som gjør at vi kan spesifisere agg funksjon dersom flere verdier til (index, col) par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eksempel på melt.. Hvis jeg har mange kolonner med verdier, eks: uke 1, uke 2, uke 3..... vil heller ha key value (en kolonne med uke, en kolonne med verdien den uke). Bare spesifiser at alle andre kolonner er id-vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df.melt(id_vars, # kombinasjon av kolonne som unik identifiserer obs\n",
    "        value_vars, # bruker alle som ikke er i id_vars som default.. kan spesifisere for å droppe resten..\n",
    "        var_name, # navn på kolonnen med verdier fra opprinnelige kolonner\n",
    "        value_name) # navn på kolonnen med verdi som korresponedere med opprinelig kolonner\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan ha tabell i stacked format. egen kolonne som hva slags type variabel og deretter kolonne med verdier,\n",
    "\n",
    "|index|type |verdi |\n",
    "|---|---|---|\n",
    "|1|a|1|\n",
    "|1|b|2|\n",
    "|2|a|3|\n",
    "\n",
    "for å få dette på tidy format der innehold i hver celle korresponderer med et (key,type) par så må jeg pivote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df.pivot(index='index', # hver unik verdi i kolonnen utgjør én rekke i ny df\n",
    "         columns='type', # hver unik verdi utgjør navn på én kolonne i ny df\n",
    "         values='verdi', # hvilke kolonner som fyller verdiene i nye kolonner\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformasjoner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vil gjøre funksjon elementvis på rekke. Gir ut array med samme størrelse der output i rekken avhenger av input i rekken. \n",
    "\n",
    "Kan ta dict som argument hvis vi vil omkode verdier i henhold til tabell\n",
    "```python\n",
    "table = {'old_val0':'new_val0', 'old_val1':'new_val1'}\n",
    "df['col'] = df['col'].map(table)\n",
    "```\n",
    "Kan også bruke lambda for å lage custom funksjoner\n",
    "```python\n",
    "df['col'] = df['col'].map(lambda x: 2*x) # x er verdi i rekken av array\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applymap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tingen er at .apply() anvender funksjon på hvert element av iterable, men har ikke helt bilde av helheten.\n",
    "\n",
    "Hvis jeg i stedet bruke .pipe() på groupby så vet den hvor mange grupper det er og sånn.\n",
    "\n",
    "hmmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kombinere dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Har fire typer joins som bestemmer hvilke observasjoner som blir med i det nye datasettet.\n",
    "1. inner\n",
    "2. left\n",
    "3. right\n",
    "4. outer\n",
    "\n",
    "for de tre nederste blir observasjon som ikke har verdi i en kolonne paddet med nans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis har samme kolonner eller samme index i to dataframes så er det enklest å bare slenge de sammen med\n",
    "\n",
    "pd.concat([liste av dfs],axis=)\n",
    "\n",
    "padder på med nans avhengig av hvilke type join. Beholder indexverdi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruker verdi i felles kolonner til å merge verdier. Tenk at vi har observasjon om hvilke land individer kommer fra. På bakgrunn av dette vil vi koble på mer informasjon om landet på hvert individ. Vi har et annet datasett med informasjon om hvert land. Kan da koble dette på våre individdata med land som 'key' kolonne. Den driter i\n",
    "\n",
    "pd.merge(left,right) tilsvarer left.merge(right)\n",
    "\n",
    "left.merge(right,on='key',how=) \n",
    "\n",
    "- Ulike navn på kolonneindex som merger på: left_on=, right_on=\n",
    "- Hvis kolonnen er plassert som index så bruker vi bare left_index=True, right_index=True. Hvis indexen har navn så kunne vi brukt left_on="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis vi vil merge to datasett med samme type index (ie. liste av navn, land .. whatever) så bruker vi pd.merger(a,b,options). Siden det ofte vil være slik at noen index ikke har verdier for noen av kolonnene har vi masse options for hvordan merge.\n",
    "\n",
    "1) how = left -- kun obs som har verdier til kol a\n",
    "\n",
    "2) how = right -- tilsvarende\n",
    "\n",
    "3) how = inner (snitt) -- verdier på begge\n",
    "\n",
    "4) how = outer (union) -- verdier på minst én\n",
    "\n",
    "Vi må også spesifisere hva som er indexene for mergingen. left_on, right_on, left_index=True etc|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convience funksjon som bruke pd.merge under the hood, men kan være bedre valg dersom vi bruker index til å koble data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legge til series (rad og kolonne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis vi bruker df.append() så lages nytt objekt. Må sende en series eller dataframe som argument. \n",
    "\n",
    "Eks: df = dt.append(pd.Series({dict: keys:values}), der keys korresponderer med kolonne indexer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan bruke bools til å indexe. Gir ut array som \"bli lagt oppå dataframen\". Får ut de verdiene som korresponderer med True. \n",
    "\n",
    "Boolean masking er viktig!! Apply operatorer til dataframe.\n",
    "\n",
    "1) Lage boolean array, f.eks: df[<column name>]<condition>. Får tilgang til kolonne og tester hver verdi opp mot condition. Returner array med samme størrelse der verdiene er enten true/false.\n",
    "\n",
    "2) \"Apply the mask to the dataframe\". Lager nå df, df_new = df.where(<mask>). Altså bruke where method og boolsk array som input. Ny df har samme størrelse som gamle, men data fra rows hvor betingelse = false ---> NaN verdier. For å droppe disse rekke kan vi bruke df = df.dropna()\n",
    "    \n",
    "\n",
    "Kan gjøre begge deler i ett steg: df_new = df[df[<kolonne>]<condition>]] Bruke boolsk array som index til original df. Jævla clean og gjør enkelt å lage mer kompliserte logiske operasjon ved å binde sammen med | (eller) & (og). Der hver del er innrammet i parantes.\n",
    "\n",
    "```python\n",
    "df_new = df[(df[[\"gold\"] > 0) & (df[\"gold1\"] == 0])]\n",
    "df_new = df[<col>][<bool mask>].\n",
    "df_new = df[df.NavnPåKolonne == verdi]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tar string som den evaluerer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan lage nye kolonne fra ifelse conditional med\n",
    "\n",
    "```python\n",
    "df['new_col'] = np.where(df['old_col']=='value', 'A', 'B')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dato-funksjonaliteter i pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidspunkt har mange egenskaper. De har rangering der større verdier er senere enn små verdier, men det er langt fra tilstrekkelig å representere denne ordinale rangeringen med tall. Det finnes mange ulike målenheter på avstand i tid (dager, sekunder, millisekunder, mm.) og det er ikke alltid trivielt å konvertere mellom disse, samt at det avhenger av tidssone. Dessuten er det mye informasjon assosiert med gitte tidspunkt (hvilken uke eller dag det er, mm).\n",
    "\n",
    "Dersom vi gir informasjon om tidspunkt en riktig representasjon kan pandas gjøre mye arbeid for oss. Det gir oss tilgang til informasjon og det kan tolke hva vi mener slik at det blir enkelt å filtrere observasjoner ut fra årstall, ukedag eller lignende. Det kan også tolke avstand mellom tidspunkt i ulike måleenheter. For å oppnå dette må vi bruke classer som er *time aware*.\n",
    "\n",
    "Av uvisse grunner har det mer intuitiv funksjonalitet når datetime array er indeks i stedet for kolonne i dataframe... Gir vel uansett stort sett mening å ha det som index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objekter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lage datetimeindex\n",
    "- pd.date_range(start, period, freq)\n",
    "- pd.period_range()\n",
    "\n",
    "Timestamp objekt, har mange egenskaper vi kan få ut\n",
    "\n",
    "Period, dag/måned/året... representer periode i stedet for snapshot. har begynnelse og slutt\n",
    "\n",
    "Timedelta for offset (differanse i tid). Kan bruke dette i algebra med tidspunkt.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan konstruere index som består av datetime med \n",
    "```python\n",
    "date_index = pd.date_range(start, # string som kan bli parset som dato..\n",
    "                           period, # int med antall perioder\n",
    "                           freq, # avstand mellom perioder, 'D' for daily, 'H' hourly, ..\n",
    "                           end, # kan alternativt spesifisere sluttdato\n",
    "                           tz # kan spesifisere timezone. Kan deretter konverte med .tz_convert('time zone')\n",
    "                           )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annen måte å konstruere er .to_datetime() fra eksisterende series med strings eller lignende\n",
    "\n",
    "```python\n",
    "pd.to_datetime(array, # prøver å parse innhold i array, men vi må ofte i praksis hjelpe litt til med flere argument\n",
    "               format, # spesifisere format, litt usikker på hvordan... eks '%Y%m%d'\n",
    "               day_first, # alternativt kan vi gi litt tips\n",
    "               )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan konstruere index av periods på måte som er analog til date_range med `pd.period_range()`\n",
    "\n",
    "representerer intervall av tidspunkt. har begynnelse og slutt. tror ulike datetime/timestamp kan være inni gitt Period. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vet forøvrig ikke hva som er forskjellen på timestamp og datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "differanse mellom datetime objekt, greit når vi skal gjøre aritmetikk.. lengde tid (duration) noe tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ikke helt trivielt å formatere string output (imotsetning til datetime objekt). Må bruke\n",
    "```python\n",
    "components = df['time_delta_col'].dt.components\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datetime interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruker .dt til å få interface til egenskaper for datetime objekt\n",
    "\n",
    "Indikere om det er helg,\n",
    "\n",
    "```python\n",
    "df['weekend'] = np.where(df['date'].dt.dayofweek < 5, 0, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vinduer for aggregering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rolling window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Litt analog til groupby, men i stedet for å konstruere egne dataframes filtrert for spesifikke verdier av variabler i angitt kolonner så vil det for hvert tidspunkt være en tilhørende dataframe med verdiene fra k tidspunkt foran.\n",
    "\n",
    "Vi kan angi aggregeringsfunksjon på disse dataframene slik at vi får én verdi tilhørende hvert tidspunkt, f.eks gjennomsnitt og varians for å undersøke stasjonaritet, eller f.eks. max/min verdi av tilhørende dataframe. Merk at dette vil gi NA for tidspunkt som ikke har nok foregående verdier til å fylle ut k tidligere tidspunkt.\n",
    "\n",
    "Den bruker bare tidligere data. Det betyr at f.eks. gjennomsnitt alltid er litt på etterskudd. Finnes alternative tilnærminger som bruker data på begge sider av det angitte tidspunktet... litt usikker på fordel og ulemper.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "r = df.rolling(window, # antall tidspunkt. Flere tidspunkt gir smoothere plot. \n",
    "               min_periods, # kan spesifisere minste antall tidligere periode for å ikke gi NA\n",
    "               center # kan velge å plassere verdi til senter av vindu...\n",
    "               )\n",
    "\n",
    "r['navn på col'].agg_func() # gir ut dataframe som vi kan plotte\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruker info fra alle observasjoner før tidspunkt, i motsetning til rolling vindu som har en angitt størrelse. Blir litt sånn som cumulative sum og sånn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan lage lagged kolonner med .shift()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis kolonne er string kan jeg bruke df.col.str for å få tilgang til metoder. Dette er ofte bedre å raskere enn å gjøre en eksplisitt loop og bruke string metode på innholdet i hver celle. Har de fleste (alle?) metodene som native python har på strings. Har i tillegg noen egne funskjonaliteter knyttet til pandas, feks:\n",
    "```python\n",
    "# lage dummmies der hver observasjon kan inneholde flere verdier i form av string\n",
    "df['col'].str.get_dummies(sep='|') \n",
    "\n",
    "# Hvis jeg vil bruke flere str operasjoner i chain må jeg bruke .str etter hver, eks\n",
    "data['Min_Salary']=data['Min_Salary'].str.strip(' ').str.lstrip('$').str.rstrip('K').fillna(0).astype('int')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diverse data cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksempler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Omdefinere verdier i kolonne i henhold til tabell/dict\n",
    "table = {'Amer-Indian-Eskimo':'Native','Asian-Pac-Islander':'Pacific'}\n",
    "df['race'] = (df['race'].\n",
    "    str.strip().\n",
    "    astype('category').\n",
    "    cat.rename_categories(table))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "# Vil ha kolonnenavn i camel_case i stedet for SnakeCase\n",
    "def camel_to_snake(s):\n",
    "    return ''.join('_'+ch.to_lower() if ch.isupper() else ch for ch in s)\n",
    "df.columns = [camel_to_snake(column) for column in df.columns]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# fjerne alle symboler i string som ikke er numerisk\n",
    "df['name'] = df['name'].map(lambda x: ''.join([ch for ch in x if ch.isdigit()]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generelle tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis vi jobber med stor dataframe og kun er interessert i liten del for å svare på et gitt spørsmål, så kan det være en god idé å lage en ny dataframe som er subset av den opprinnelige. \n",
    "\n",
    "Stiltips:\n",
    "\n",
    "1) Unngå ][ , chain indexing\n",
    "\n",
    "2) Chain metoder. En måte å gjøre dette mer oversiktelig er å fordele det over flere linjer (analog til dplyr), eks:\n",
    "\n",
    "```python\n",
    "(df.groupby[\"NN\"]\n",
    "    .MM # velge kolonne\n",
    "    .dropna() # kommentar\n",
    "    .mean() # kommentar\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pd.cut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når vi vil dele kontinuerlige data inn i bins. Konstruere 'factor' med levels, kan deretter lage dummies\n",
    "\n",
    "```python\n",
    "pd.cut(array,\n",
    "       bins, # Antall bins (lager slik at uniform antall i hver) eller konstruere med lik avstand mellom\n",
    "       retbins=True, # spesifisere om returnere cutoffs til bins\n",
    "       labels # kan bruke np.range(len(bins)) hvis kun interessert i relativ plassering\n",
    "       )\n",
    "```\n",
    "\n",
    "bins kan ta ulike form:\n",
    "1. int, spesifiserer antall bins, partisjonerer (min(x),max(x)) inn i like store intervall\n",
    "2. sekvens av tall (a,b,c,..), lager intervall (a,b],(b,c] hvis right=True. Gir NaN hvis ikke i angitte intervall.\n",
    "3. IntervalIndex (vet ikke)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Andre typer objekt i pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kjenner til groupby-objekt. Finnes også andre objekt som vi kan utføre aggregeringsfunksjoner på og få ut Series eller df.\n",
    "\n",
    "-  a = pd.Series(..), a.rolling(window=antall obs) <- gir oss et rolling objekt med en rekke metoder.\n",
    "-  a.mean() gir oss en Series med rullende gjennomsnitt. Denne seriesen kan vi f.eks. bruke til å plotte.\n",
    "-  Et alternativ til aritmetisk gjennomsnitt er at tyngden til observasjon er vektet etter tid. Kan bruke a.ewm() til å få eksponentielt vektet ting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nyttige metoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  a.pct_change() gir oss en ny series (eller df hvis flere kolonner) der hver observasjon fra i=1 -> n er erstattet med df[i]/df[i-1]-1. Husk å gange med 100 for å få prosentvis endring\n",
    "- a.nlargest() gir series med index og verdi til n største verdiene i en series\n",
    "- a.transform(func) gir en series med samme størrelse der vi har kjørt en funksjon på den.. Kan også brukes på dataframes. Uansett, eksempel er b = a.transform(lambda x: pd.cut(x, 100))\n",
    "- a.str gir oss tilgang til mange string metoder som vi kan anvende på alle elementer i series. raskere enn å kjøre det i en loop.\n",
    "- df.iterrows() gir oss et generatorobjekt som vi kan bruke til å loope oss gjennom df row by row. Hvert element er en tuple med rowindex og rowen som series. Følger da at index til min row series er kolonnenanvnene. \n",
    "- agg og apply, vet ikke helt hva som er forskjellen\n",
    "- stack gjøre kolonnenavn til indexverdier\n",
    "- unstack gjør indexverdier til kolonnenavn\n",
    "- reindex til å velge subset av index/kolonne når jeg har liste av tall jeg vil subsette med\n",
    "- idxmax() gir første index til høyeste verdi langs gitt index.. hvis axis=1 så får vi kolonnen med høyest verdi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nyttige funksjoner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ufuncs fungerer også på dataframe\n",
    "- Har innebygde funksjoner som gir én output per kolonne/rekke (typ statistiske/matematiske funksjoner)\n",
    "- pd.get_dummies() lager dummy dataframe fra kategorisk variabel. Kan spesifisere predix=\"string\" slik at de får kolonnenavn [\"string_0\",\"string_1,...]\n",
    "- Kan også bruke det dersom vi bare vil ha én kolonne, df['col'] = pd.get_dummies(df.col)[val], der val er verdi av kol som vi vil skal ta verdi 1 (resten 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det er ganske greit å rekonstruere en kategorisk variabel fra dummies (f.eks for plotting purposes eller hvis jeg vil kjøre groupby på det). Kategorisk er egentlig alltid en bedre representasjon før vi kjører det inn i algoritme, så synes egentlig ikke jeg bør se noen fuckings dummies okay det dritet der bør det være mulig å skjule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "subset_cols = [col for col in df.columns if col.startswidth('prefix')]\n",
    "subset_df = df[subset_cols]\n",
    "new_col = subset_df.idxmax(axis=1) # får nye series der tar kolonnenavn (fra kol der val=1) som verdi\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
