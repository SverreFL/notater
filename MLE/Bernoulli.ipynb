{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sympy import *\n",
    "import sympy.stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X\\sim bernoulli(\\rho)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plukker baller fra urne med tilbakelegging slik at hvert trekk er uavhengig. Har et utfallsrom $\\Omega = \\{\\text{rød, blå, gul, grønn}\\}$. Definerer tilfeldig variabel $X(\\omega) = I\\{\\omega = \\text{rød}\\}$. Fordelingen $\\mathcal{L}(X)=P$ er nødvendigvis bernoulli-fordelt. Kan beskrive sannsynlighet for alle delmengder av utfallsrom $Z = \\{0,1\\} \\subset \\mathbb{R}$ med enkelt parameter $\\rho := P(\\{X=1\\})$ ettersom $P(\\{X=0\\}) = 1-\\rho$ og delmengdene er disjunkte slik at $P(\\{X=0\\}\\cup \\{X=1\\}) = (1-\\rho)+\\rho = 1$. Merk at vi kan tenke på $\\rho$ som andelen av ballene i urnen som er røde.\n",
    "\n",
    "Vi trenger derfor ikke gjøre noe modellering (antagelser som avgrenser fordelinger vi betrakter) når vi sier at $P \\in \\mathscr{P}_{\\theta} = \\{P_{\\theta}:\\theta \\in \\Theta\\}$, der $\\theta = \\rho$ og $\\Theta = [0,1]$ i vårt eksempel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anta nå at vi ikke kjenner andelen røde baller, men har lyst til å estimere dette. For å lære om fordeling til $X$ må vi observere noen realiseringer. Vi trekker $N$ baller med tilbakelegging. Intuitivt vil det være rimelig å anta at dersom en stor del av ballene vi observerer er røde så vil også en stor andel av ballene i urnen være røde, og omvendt. For å formalisere denne intuisjonen kan vi innføre likelihoodfunksjonen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hver fordeling $P_{\\theta}:\\mathcal{B}(Z) \\mapsto [0,1]$ er det en cdf $F_{\\theta}:Z\\mapsto [0,1]$ der $F(x) = P[(-\\infty,x)]$ dersom $Z \\subset \\mathbb{R}$. Den kumulative fordelingen korresponderer med pmf eller pdf avhengig av om fordeling er diskret eller kontinuerlig. $f(x) =F'(x) \\approx \\frac{F(x+dx)-F(x)}{dx} \\implies f(x)dx \\approx F(x+dx)-F(x) = P[(x,x+dx)]$\n",
    "\n",
    "$p(x) = F(x)-\\lim_{s\\to x^-}F(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dersom funksjonene er fult ut spesifisert kan vi beregne sannsynlighet for utfall til $X$.\n",
    "\n",
    "$$P(A\\subset Z\\subset \\mathbb{R}) = \n",
    "\\begin{cases}\n",
    "\\sum_{x \\in A} p(x) \\\\\n",
    "\\int_{x \\in A} f(x) dx\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pmf og cdf tilhører ofte parametrisk familie slik at $p(x,\\theta)$. Når vi betrakter det som fordeling holder vi $\\theta$ konstant og betrakter kun $x$ som variabel. Likelihoodfunksjonen er identisk, men nå holder vi $x$ konstant og ser hvordan funksjonen endres når vi varier $\\theta$. For hver $\\theta \\in \\Theta$ angir likelihoodfunksjonen sannsynligheten for å observere $x \\in Z$ gitt at $\\mathcal{L}(x) = P_{\\theta}$, altså dersom den er generert fra fordeling med gitte parameter. Det er ikke sannsynlighetsfordeling fordi funksjon ikke tilfredstiller aksiom, $\\int{\\theta \\in \\Theta} L(\\theta) \\neq 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeg bruker likelihood-funksjon til å velge punktestimat $\\widehat{\\theta}$ som gjør det mest sannsynlig å ha observert de realiserte verdiene i utvalget. Med bernoulli gir dette\n",
    "\\begin{align}\n",
    "&L_n(\\rho) = \\rho^{X_n}(1-\\rho)^{1-X_n} \\\\\n",
    "&L(\\rho) = \\Pi_n L_n(\\rho) = \\Pi_n \\rho^{X_n}(1-\\rho)^{1-X_n}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I praksis har vi et utvalg med realiserte verdier $(X_1 = x_1, X_2=x_2,...,X_N=x_n) := (x_1,x_2,...,X_N) \\in \\mathbb{R}^N$. Da er alt bare tall og vi kan maksimere funksjonen. Vi gjør først en logaritimisk transformasjon som er monoton og dermed beholder argmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "&logL_n(\\rho) = X_n \\log(\\rho)+(1-X_n)\\log(1-\\rho)\\\\\n",
    "&logL(\\rho) = \\Sigma logL_n(\\rho) = \\Sigma X_n \\log(\\rho)+(1-X_n)\\log(1-\\rho)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dette reduserer MLE til et ganske enkelt numerisk optimeringsproblem der vi for et gitt uvalg $\\mathbf{x}\\in \\mathbb{R}^N$ finner $\\widehat{\\theta} = \\arg \\max_{\\rho \\in \\Theta}L(\\theta;\\mathbf{x})$. Det gir punktestimat for vårt gitte utvalg, men teorien er mye rikere enn dette. Formen på likelihoodfunksjonen sier oss hvor mye vi kan forvente å lære om parameter $\\theta$ fra å observere én realisering av $X$ der $\\mathcal{L}(X) = P_{\\theta}$. Dette er egenskap ved modellen $P_{\\theta}$; ikke om vårt gitte utvalg. Det kan derfor være nyttig å betrakte situasjonen \\textit{før} vi observerer de $N$ realiseringene av $X$. Da er $logl_n(\\cdot;X_n)$ en tilfeldig funksjon $logl_n : \\Theta \\mapsto \\mathbb{R}$. \n",
    "\n",
    "Tror vi kan tenke at det eksisterer en funksjon $g: Z \\subset \\mathbb{R} \\mapsto S = \\{logl_n(\\cdot;x_n):x_n \\in Z\\}$. \n",
    "\n",
    "Uansett, evaluert i $\\theta$ er verdien av funksjonen, $logl_n(\\theta;X_n), \\in \\mathbb{R}$ slik at det er en tilfeldig variabel og kan analyseres som dette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dette medfører at parameter er argmax til forventningsverdi av likelihood. Vet ikke helt hvordan jeg skal motivere og tenke på det. Det medfører også at jeg finne et utrykk for den ved å sette forventningsverdi av score lik 0. Mer om dette og konkret eksempel ved bernoulli senere."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
