{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datastrukturer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bygger på numpy arrays og arver mye funksjonalitet derfra. Det er to sentrale objekt: Series og DataFrame\n",
    "\n",
    "I tillegg til den implisitte numeriske indexen til arrays har den også et eksplisitt indexobjekt som mapper til element i Series eller til Series i DataFrame. Koblingen mellom indeks og verdi gjør at vi kan kombinere data fra ulike kilder, håndtere missing data og generelt ikke er avhengig av å ha helt konforme data (samme størrelse, samme rekkefølge) for å gjøre regneoperasjoner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series er crosseover mellom array og dict. Består av to arrays: én index og én med homogene data. Vanlige ndarrays har implisitt index med posisjon, men her er det eksplisitt og kan ha ulike labels. Kan også tenke på Series som en dict som mapper index-verdier til verdi i array. Følgelig er det en del metoder/syntax som tilsvarer dict. Kan få ut hver av arraysene med:\n",
    "1. a.values (ndarray)\n",
    "2. a.index (index-objekt, finnes litt ulike typer, har diverse metoder, noe mengdegreier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame er tabulær datastruktur med både rekke-index og kolonne-index. I utgangspunktet er det ganske symmetrisk, men av konvensjon angir rekke-index obseravjon og kolonne-index er variabel (dimensjon/egenskap) ved observasjonen. Kan betrakte det som en dict som mapper label til Series, der labels er enten fra rekke- eller kolonneindex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det er en fullverdi datastruktur/objekt i seg selv. Har en del metoder og sånn. Vet ikke om jeg må jobbe så mye direkte med indexen; kan gjøre operasjon på dataframe og så håndterer de index internt.\n",
    "\n",
    "Kan bruke eksisterende kolonne som index med df.set_index('kolnavn')\n",
    "\n",
    "Merk at når vi setter en ny kolonne som indeks mister vi den eksisterende index-kolonnnen. Hvis vi vil beholde må vi lagre kopi av kolonne og deretter putte den inn i nye dataframe. Kan få tak i kolnnen ved å bruke df.index() og deretter dytte den inn i df ved å assigne kolonnen med df[\"navn\"] = df.index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Indeksing\n",
    "\n",
    "Kan ha fler dimensjonale index, f.eks først by state i kol 1, deretter kommune i kol 2. Begge indexer. Hierarkisk labeling.\n",
    "\n",
    "Lager ved å gi liste av liste som argument inn i df.setindex\n",
    "\n",
    "eks: df = df.setindex([\"kol1\",\"kol2\"])\n",
    "\n",
    "For å finne gitt rekke må vi gi df.loc[\"index1\",\"index2\"]\n",
    "\n",
    "Kan endre navn på index ved .rename(columns = {\"gammel\":\"ny}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kolonner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan merke at kolonnene egentlig bare er index og at det i utgangspuntket er ganske symmetrisk.\n",
    "\n",
    "Vil ha deskriptive kolonnenavn. Bruker\n",
    "- df.rename(columns=mapper), der mapper er dictionary med {'gammel':'ny',:}\n",
    "\n",
    "Tror min konvensjon er å ha navn i snake_case, kan bruke\n",
    "\n",
    "```python \n",
    "def camel_to_snake(s):\n",
    "    return ''.join('_'+ch.to_lower() if ch.isupper() else ch for ch in s)\n",
    "df.columns = [camel_to_snake(column) for column in df.columns]\n",
    "\n",
    "```\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan bruke vanlig list comprehension til å finne subset av kolonner som oppfyller kriterie,\n",
    "\n",
    "```python\n",
    "cols_subset = [col for col in df.columns if 'string' in col]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarkisk index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan ha flere nivå på index. Det gjør at vi kan representere data som i utgangspunktet er i høyere dimensjon som tabulær data (eks: panel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi index\n",
    "\n",
    "Finnes eget MultiIndex objekt. Lager ved å sende liste inn i df.setindex()\n",
    "\n",
    "-  Kan f.eks. få ut df med multiindex hvis vi kjører en groupby på flere kolonner. Aggregerer da på subset av observasjon som har kol1 = A og kol2 = B f.eks. \n",
    "-  Kan få ut relativ andel av kategori som har gitt verdi av underkategori ved å lage groupby på multiindex, df.groupby(level=1).sum() .\n",
    "- Mer at groupby lager delmengder av observasjon med gitt verdi av index. Kan eksplisitt loope over dette:\n",
    "\n",
    "for idx, gb in groupby(level=1):\n",
    "    \n",
    "    df_sub = gb\n",
    "    \n",
    "Kan få ut index til hvert nivå med \n",
    "df.index.get_level_values(num), der 0 er øverste nivå."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slice multiindex\n",
    "\n",
    "For å få økt fleksibilitet er liste og tuple ulik tolkning\n",
    "\n",
    "- df.loc[id11]: all rows where the outer most index value is equal to id11\n",
    "- df.loc[(id11, id21)]: all rows where the outer-most index value is equal to id11 and the second level is equal to id21\n",
    "- df.loc[[id11, id12]]: all rows where the outer-most index is either id11 or id12\n",
    "- df.loc[([id11, id12], [id21, id22]), :]: all rows where the outer-most index is either id11 or id12 AND where the second level index is either id21 or id22\n",
    "- df.loc[[(id11, id21), (id12, id22)], :]: all rows where the the two hierarchical indices are either (id11, id21) or (id12, id22)\n",
    "\n",
    "Dersom vi vil begrense oss til noen få indre verdier, men alle ytre, så trenger vi IndexSlice objekt som argument for vår loc\n",
    "\n",
    "- pd.IndexSlice[:,['A', 'B'],:] gir oss alle ytre, 'A' og 'B' fra nivå innenfor, og verdier fra alle kolonner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sortering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan sortere entent etter index eller verdi langs gitt kolonne,\n",
    "1. df.sort_index()\n",
    "2. df.sort_values(by='kolnavn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lage dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fra fil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spesifiserer hvordan pd.readcsv() skal laste inn data\n",
    "\n",
    "1. usecols = [..] for å spesifisere subset av kolonner vi vil laste inn\n",
    "2. names = [..] hvis kolonnene ikke har navn fra før så kan vi spesifisere de.\n",
    "3. Bestemme hvilken kolonne i datasett som skal fungere som index, index_col = <tall>. \n",
    "4. Dersom det er rekke vi ikke vil laste inn, skiprows = <tall>, skipper de n øverste.\n",
    "5. parse_dates = True ---> finner kolonne med date, gjør den til index, får den i datetime.\n",
    "    \n",
    "Kan også laste inn fra andre filtyper enn .csv, for eksempel .json. Tar da orient keyword med verdi \"split\", \"records\" eller \"columns\"\n",
    "    \n",
    "pd.read_json(orient=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fra datastrukturer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan ha lyst til å lage tabell som mapper index til verdi. F.eks. tabell med som mapper variabelnavn til reg koef. Har to lister. Et alternativ er pd.Series(data=x,index=y). Annet alternativ er å først konstruere dict:\n",
    "\n",
    "pd.Series(dict(zip(y,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beskrive data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Før jeg kan gjøre noe analyse må jeg først undersøke tabellen med tall jeg har fått utdelt. Må få oversikt over:\n",
    "1. Hvilke egensakper (kolonner) og hva de måler\n",
    "2. Hva slags måleenhet de er i og konvertere til riktig datatype. I algebraen under the hood er alt tall, men representer som factor for å få bedre representasjon og utnytte funksjonalitet i plotte-libraries og lignende. Vil unngå å jobbe eksplisitt med dummies før det er nødvendig.\n",
    "3. Få noe oversikt over univariate fordelinger og eventuelt parvis bivariate korrelasjoner. Se at tall er rimelig, blir kjent med data, oppdage numerisk kodete missing values, se etter feilkodinger\n",
    "4. Få oversikt over manglende verdier, vurdere strategi for imputation og eventuelt bias dersom vi ser på delutvalg med fullstendig obervasjon.\n",
    "\n",
    "Kan bruke .describe() --- output avhenger av datatype\n",
    "\n",
    "Kan bruke .info() til å få datatype og observere om det er missing values\n",
    "\n",
    "Kan bruke .ndtype() for å finne datatypene\n",
    "\n",
    "Hvis vi har en series kan vi bruke .value_count() for å få antallet observasjoner med gitt verdi langs den dimensjonen.\n",
    "\n",
    "Kan bruke .unique() for å få liste av unike verdier og .unique() for antallet unike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datatyper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hva slags type data vi har påvirker gyldige operasjoner og analyse av data. Derfor viktig å kategorisere. Hver kolonne er homogen.\n",
    "\n",
    "1. Ulike numeriske datatyper (int,float,..)\n",
    "2. category\n",
    "3. object\n",
    "\n",
    "Kan bestemme datatype i series(kolonne) i df ved:\n",
    "\n",
    "df.kol1.astype('category', ordered=True)\n",
    "\n",
    "Kan endre datatype til flere kolonner ved : df[liste_av_kolonner] = df[liste_av_kolonner].astype('dtype') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beskrive kolonner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruker value_counts(normalize=).sort_index() # normalize hvis jeg vil ha som prosent\n",
    "\n",
    "gir oversikt over antall unike verdier, hvor tyngden av data ligger og om det er verdier som ikke gir mening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vil representere med NaN. Kan lage med np.nan. Dette er en float, slik at kolonner med missing values blir recast til floats. Det er litt teknisk hvordan det blir representert under the hood, får eventuelt se på det senere. I R er det na og null, i pandas bare NaN (finnes en mer moderne datatype na som de har lagt til).\n",
    "\n",
    "Missing values propapegerer når vi gjør operasjoner på dataframe. Operasjon er elementvis og alt som bruker NaN resultererr i NaN. Dette gjelder også får aggregeringsfunksjoner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Første jeg må gjøre er å sjekke antallet missing values i ulike kolonner. Husk at missing values kan være kodet på ulike måter i ulike datasett. En mulighet er å bruke df['col'].unique() til å se på verdier og se om noen ikke gir mening. Deretter vil jeg omkode de til NaN, f.eks. ved df.replace(value,np.nan)\n",
    "\n",
    "- df.isna() gir boolean mask over hele dataframe\n",
    "- df.isna().any() angir om hvorvidt det er nans i hver av kolonner\n",
    "- df.isna().sum() gir antallet nans i hver kolonne\n",
    "\n",
    "Når jeg har fått oversikt kan jeg vurdere å filtrere de ut. Bruker da df.dropna() med opsjoner\n",
    "- axis= .. droppe kolonne eller rekke avhengig om jeg finner nans der\n",
    "- how='any','all'\n",
    "- tresh= antall nans før jeg dropper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En alternativ fremgangsmåte er å fylle inn verdier. Algoritmer i sklearn kan ikke håndtere nans og det er synd å kaste bort data bare fordi noen observasjoner er litt mangefulle. Finnes ulike strategier for dette som jeg kan se på senere\n",
    "1. Bruke gjennomsnitt i kolonnen\n",
    "2. Predikere verdi ut fra andre dimensjoner vi observere\n",
    "3. Bruke verdi fra naboer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays i dataframe er homogone. Dersom det er noen symboler i kolonne som i utgangspunktet skal være numerisk vil pandas tolke det som string. Må fjerne symbolene før vi kan konvertere.\n",
    "```python\n",
    "def to_numeric(s):\n",
    "    return int(''.join(ch for ch in s if ch.isnumeric()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split-apply-combine workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi har et stort datasett. Det er mange variabler og mange kolonner. Hvordan skal vi få ut noen innsikter fra all denne informasjonen?\n",
    "\n",
    "Kan vært veldig nyttig å dele det inn i mindre deler etter felles verdi langs kolonne. Kan tenke at det er observasjoner som \"hører sammen\". Deretter annvender vi noen aggregeringsfunksjoner som gir oss slags sammendragsmål for tallverdiene i hver del. Deretter kan vi kombinere det sammen i nytt datasett som mapper nøkkel (felles verdi) til verdi av aggregeringsfunksjon.\n",
    "\n",
    "Okay, la oss se på dette i praksis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gbA = df.groupby('A') gir oss et groupby objekt. Det er poeng at hver gruppe er en dataframe. Kan få ut disse dataframene med gbA.get_group(verdi av A). Kan gruppere med verdier langs flere kolonner; dette gir og PxK grupper, der P er verdi langs første og K er verdi langs andre. I den kombinere dataframen vil har multiindex. Kan også observere at hver observasjon er i én gruppe.\n",
    "\n",
    "Kan anvende aggregeringsmetoder direkte på gbA. Kan også definere custom aggregeringfunksjon og anvende de med .agg metoden\n",
    "\n",
    "gbA.agg(lambda x: (x > 0).count()) gir antaller positive observasjon. Kan være greit å gi funksjonene navn så blir litt mer ryddig.\n",
    "\n",
    "For å øke fleksibiliteten til vår oppdeling av datasett kan vi lage et Grouper objekt. Stor fordel her Grouperen vår kan tolke datetime objekter. Hvis key=series med datetime så kan vi slenge in freq=\"W\" og gruppere observasjoner for hver uke\n",
    "\n",
    "eksempel: gb = df.groupby(pd.Grouper(key='Date', freq ='W')) der Date er kolonne med datetime objekt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .agg() metoden\n",
    "\n",
    "Hvis vi har en groupby objekt og vil finne aggegert tallmål på verdiene i annen kolonne assosiert med hver verdi i groupby\n",
    "\n",
    "eks: df.groupby(\"Kol1\").agg({\"kol2\": np.mean}) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply og applymap\n",
    "\n",
    "Kan ha lyst til å lage custom funksjoner gir ett output per kolonne/rekke. Definerer egen funksjon, f = lambda x: .., der x er series, og tar deretter df.apply(f)\n",
    "\n",
    "Ofte vil jeg lage en kolonne der verdi avhenger verdi observasjon har langs annen kolonne. Påfører funksjon elemntvis, men kan vektorisere med map. \n",
    "\n",
    "eks: df['ny_col']=df['col'].map(lambda x: 2*x)\n",
    "\n",
    "F.eks hvis vi vil generere en variabel som er en funksjon av verdiene av to korresponderende verdier i to iterables\n",
    "\n",
    "f.eks: x3i = x1i*x2i , i= 1, ... , n\n",
    "\n",
    "x3 = map(lambda x,y:x*y,x1,x2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lage en df fra eksisterende df for å analysere spesifikk problem. Bestemmer index. Lar verdiene i én av kolonnene være kolonner(kategorier) i ny df. Bestemmer hvilken kategori som skal være verdi for hver index langs nye kolonner. Spesifisere aggregeringsfunksjon for alle verdiene for gitt katagori for hver index (flere observasjoner inn i samme rute --> aggregering til ett tall).\n",
    "\n",
    "eks: df.pivot_table(values=BNP , index=ÅR ,columns =LAND, agg=np.mean) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Vi kan representerer de samme dataene på ulike måter. \n",
    "\n",
    "1. Long (alt er i index...)\n",
    "2. Wide (alt er i kolonne...)\n",
    "\n",
    "Vi har fire grunnoperasjon for å omforme data\n",
    "\n",
    "1. stack (sender ting ned i index, flere levels på indexen)\n",
    "2. unstack (sender ting opp i kolonne, mindre levels på indexen)\n",
    "3. set_index\n",
    "4. reset_index\n",
    "\n",
    "Har operasjoner som bruker disse under the hood\n",
    "\n",
    "1. melt (gjør long)\n",
    "2. pivot (index, col, value) <- wide?\n",
    "3. pivot_table, generalisering som gjør at vi kan spesifisere agg funksjon dersom flere verdier til (index, col) par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eksempel på melt.. Hvis jeg har mange kolonner med verdier, eks: uke 1, uke 2, uke 3..... vil heller ha key value (en kolonne med uke, en kolonne med verdien den uke). Bare spesifiser at alle andre kolonner er id-vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan ha tabell i stacked format. egen kolonne som hva slags type variabel og deretter kolonne med verdier,\n",
    "\n",
    "|index|type |verdi |\n",
    "|---|---|---|\n",
    "|1|a|1|\n",
    "|1|b|2|\n",
    "|2|a|3|\n",
    "\n",
    "for å få dette på tidy format der innehold i hver celle korresponderer med et (key,type) par så må jeg pivote\n",
    "\n",
    "df.pivot(index=index,columns=type,values=verdi)\n",
    "\n",
    "Tenker at index + columns må identifisere unik observasjon, deretter bruke values kolonne til å fylle inn verdier i columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kombinere dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Har fire typer joins som bestemmer hvilke observasjoner som blir med i det nye datasettet.\n",
    "1. inner\n",
    "2. left\n",
    "3. right\n",
    "4. outer\n",
    "\n",
    "for de tre nederste blir observasjon som ikke har verdi i en kolonne paddet med nans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis har samme kolonner eller samme index i to dataframes så er det enklest å bare slenge de sammen med\n",
    "\n",
    "pd.concat([liste av dfs],axis=)\n",
    "\n",
    "padder på med nans avhengig av hvilke type join. Beholder indexverdi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruker verdi i felles kolonner til å merge verdier. Tenk at vi har observasjon om hvilke land individer kommer fra. På bakgrunn av dette vil vi koble på mer informasjon om landet på hvert individ. Vi har et annet datasett med informasjon om hvert land. Kan da koble dette på våre individdata med land som 'key' kolonne. Den driter i\n",
    "\n",
    "pd.merge(left,right) tilsvarer left.merge(right)\n",
    "\n",
    "left.merge(right,on='key',how=) \n",
    "\n",
    "- Ulike navn på kolonneindex som merger på: left_on=, right_on=\n",
    "- Hvis kolonnen er plassert som index så bruker vi bare left_index=True, right_index=True. Hvis indexen har navn så kunne vi brukt left_on="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis vi vil merge to datasett med samme type index (ie. liste av navn, land .. whatever) så bruker vi pd.merger(a,b,options). Siden det ofte vil være slik at noen index ikke har verdier for noen av kolonnene har vi masse options for hvordan merge.\n",
    "\n",
    "1) how = left -- kun obs som har verdier til kol a\n",
    "\n",
    "2) how = right -- tilsvarende\n",
    "\n",
    "3) how = inner (snitt) -- verdier på begge\n",
    "\n",
    "4) how = outer (union) -- verdier på minst én\n",
    "\n",
    "Vi må også spesifisere hva som er indexene for mergingen. left_on, right_on, left_index=True etc|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convience funksjon som bruke pd.merge under the hood, men kan være bedre valg dersom vi bruker index til å koble data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legge til series (rad og kolonne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis vi bruker df.append() så lages nytt objekt. Må sende en series eller dataframe som argument. \n",
    "\n",
    "Eks: df = dt.append(pd.Series({dict: keys:values}), der keys korresponderer med kolonne indexer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan bruke bools til å indexe. Gir ut array som \"bli lagt oppå dataframen\". Får ut de verdiene som korresponderer med True. \n",
    "\n",
    "Boolean masking er viktig!! Apply operatorer til dataframe.\n",
    "\n",
    "1) Lage boolean array, f.eks: df[<column name>]<condition>. Får tilgang til kolonne og tester hver verdi opp mot condition. Returner array med samme størrelse der verdiene er enten true/false.\n",
    "\n",
    "2) \"Apply the mask to the dataframe\". Lager nå df, df_new = df.where(<mask>). Altså bruke where method og boolsk array som input. Ny df har samme størrelse som gamle, men data fra rows hvor betingelse = false ---> NaN verdier. For å droppe disse rekke kan vi bruke df = df.dropna()\n",
    "    \n",
    "\n",
    "Kan gjøre begge deler i ett steg: df_new = df[df[<kolonne>]<condition>]] Bruke boolsk array som index til original df. Jævla clean og gjør enkelt å lage mer kompliserte logiske operasjon ved å binde sammen med | (eller) & (og). Der hver del er innrammet i parantes.\n",
    "\n",
    "Eksempel: df_new = df[(df[[\"gold\"] > 0) & (df[\"gold1\"] == 0])]\n",
    "\n",
    "Eksempel 2: df_new = df[<col>][<bool mask>]. Gir verdiene fra kolonne der bool mask (som kanskje kommer fra annen kolonne i df) har verdi True. \n",
    "\n",
    "Eksempel 3: df_new = df[df.NavnPåKolonne == verdi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dato-funksjonaliteter i pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det er to hoved-classer i pandas for å behandle tid-data\n",
    "\n",
    "1) Timestamp --- spesifikk tidspunkt\n",
    "\n",
    "2) Period -- for intervall av tidspunkt\n",
    "\n",
    "i tillegg egen klasse for tid som indeks i series.\n",
    "\n",
    "Bruker .to_datetime() for å konvertere liste med representasjoner av tid til datetime.\n",
    "\n",
    "Vanligvis har vi tiden oppgitt som string, må legge inn \"kode\" for hvordan den konverteres til datetime som argument\n",
    "\n",
    "eks: format = '%Y%m%d'\n",
    "\n",
    "Tror dette bygger på datetime library som har i hvertfall to classer:\n",
    "\n",
    "1) datetime\n",
    "\n",
    "2) timedelta, er differanse mellom datetime objekt, greit når vi skal gjøre aritmetikk\n",
    "\n",
    "har masse metoder som gjør de enklere å regne med. har mange flere funksjonaliteter enn en string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diverse data cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fjerne sjit fra en liste av strings: bruker list comprehension for å generere en ny liste der hver string erstattes med string.replace(\"\",\"\")\n",
    "\n",
    "eks : a = [i.replace(\"-\",\"\") for i in liste] # litt problem at vi mister index.. av diverse årsaker vil jeg alltid beholde index i stedet for å få en ren array\n",
    "\n",
    "Annen metode er å bruke slicing dersom det er mønster i dataene vi kan utnytte\n",
    "\n",
    "Hvis vi vil fjerne alle symboler som ikke er tall fra strings kan vi bruker\n",
    "\n",
    "df['name'] = df['name'].map(lambda x: ''.join([ch for ch in x if ch.isdigit()]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis kolonne er string kan jeg bruke df.col.str for å få tilgang til metoder. Dette er ofte bedre å raskere enn å gjøre en eksplisitt loop og bruke string metode på innholdet i hver celle. Har de fleste (alle?) metodene som native python har på strings. Har i tillegg noen egne funskjonaliteter knyttet til pandas, feks:\n",
    "\n",
    "- df['col'].str.get_dummies(sep='|'), lage dummmies der hver observasjon kan inneholde flere verdier i form av string\n",
    "\n",
    "Hvis jeg vil bruke flere str operasjoner i chain må jeg bruke .str etter hver, eks\n",
    "- data['Min_Salary']=data['Min_Salary'].str.strip(' ').str.lstrip('$').str.rstrip('K').fillna(0).astype('int')\n",
    "\n",
    "a.split('x',maxsplits=n), splitter på de n første tilfellene av 'x'. Fjerner symbolet som det splittes på. Hvis jeg vil ha ut string innhold før split bruker jeg a.split('x',maxsplits=n).str[0] ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generelle tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis vi jobber med stor dataframe og kun er interessert i liten del for å svare på et gitt spørsmål, så kan det være en god idé å lage en ny dataframe som er subset av den opprinnelige. \n",
    "\n",
    "Stiltips:\n",
    "\n",
    "1) Unngå ][ , chain indexing\n",
    "\n",
    "2) Chain metoder. En måte å gjøre dette mer oversiktelig er å fordele det over flere linjer (analog til dplyr), eks:\n",
    "\n",
    "```python\n",
    "(df.groupby[\"NN\"]\n",
    "     .MM # velge kolonne\n",
    "    .dropna() # kommentar\n",
    "    .mean() # kommentar\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pd.cut()\n",
    "\n",
    "Når vi vil dele kontinuerlige data inn i bins. Konstruere 'factor' med levels, kan deretter lage dummies\n",
    "\n",
    "pd.cut(x=kolonne,,right=True,bins=,labels=,..)\n",
    "\n",
    "bins kan ta ulike form:\n",
    "1. int, spesifiserer antall bins, partisjonerer (min(x),max(x)) inn i like store intervall\n",
    "2. sekvens av tall (a,b,c,..), lager intervall (a,b],(b,c] hvis right=True. Gir NaN hvis ikke i angitte intervall.\n",
    "3. IntervalIndex (vet ikke)\n",
    "\n",
    "som default blir output en kolonne som viser hvilket intervall observasjon tilhører. Kan gi labels som må være iterable med samme lengde som antall bins for å gi annen representasjon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pd.cut(series, bins) gir oss en series med index til series og verdier indelt i et gitt antall bins. Blir litt som et histogram. Deler intervallet inn i like store deler. Med default får vi ut hvilke intervall verdien av variabelen er innenfor. \n",
    "Hvis vi setter labels=False får vi ut hvilket posisjon den har i rekkefølge av intervaller.\n",
    "- pd.qcut( ) har tilsvarende atferd som over, men i stedet for å dele intervallet i like store delintervall så deler den det i intervall der det er like mange observasjoner innenfor hver.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan lage lagged kolonner med .shift()\n",
    "\n",
    "Hvis jeg vil summe alle elementene i en dataframe så er det bare å kjøre df.sum().sum(), første gir series, andre summer opp series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Andre typer objekt i pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kjenner til groupby-objekt. Finnes også andre objekt som vi kan utføre aggregeringsfunksjoner på og få ut Series eller df.\n",
    "\n",
    "-  a = pd.Series(..), a.rolling(window=antall obs) <- gir oss et rolling objekt med en rekke metoder.\n",
    "-  a.mean() gir oss en Series med rullende gjennomsnitt. Denne seriesen kan vi f.eks. bruke til å plotte.\n",
    "-  Et alternativ til aritmetisk gjennomsnitt er at tyngden til observasjon er vektet etter tid. Kan bruke a.ewm() til å få eksponentielt vektet ting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nyttige metoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  a.pct_change() gir oss en ny series (eller df hvis flere kolonner) der hver observasjon fra i=1->n er erstattet med df[i]/df[i-1]-1\n",
    "- Kan vi bruke dette til å finne aggregert avkastning? Ja. Men hvordan?? hmm???\n",
    "- a.nlargest() gir series med index og verdi til n største verdiene i en series\n",
    "- a.transform(func) gir en series med samme størrelse der vi har kjørt en funksjon på den.. Kan også brukes på dataframes. Uansett, eksempel er b = a.transform(lambda x: pd.cut(x, 100))\n",
    "- a.str gir oss tilgang til mange string metoder som vi kan anvende på alle elementer i series. raskere enn å kjøre det i en loop.\n",
    "- df.iterrows() gir oss et generatorobjekt som vi kan bruke til å loope oss gjennom df row by row. Hvert element er en tuple med rowindex og rowen som series. Følger da at index til min row series er kolonnenanvnene. \n",
    "- agg og apply, vet ikke helt hva som er forskjellen\n",
    "- stack gjøre kolonnenavn til indexverdier\n",
    "- unstack gjør indexverdier til kolonnenavn\n",
    "- reindex til å velge subset av index/kolonne når jeg har liste av tall jeg vil subsette med\n",
    "- idxmax() gir første index til høyeste verdi langs gitt index.. hvis axis=1 så får vi kolonnen med høyest verdi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nyttige funksjoner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ufuncs fungerer også på dataframe\n",
    "- Har innebygde funksjoner som gir én output per kolonne/rekke (typ statistiske/matematiske funksjoner)\n",
    "- pd.get_dummies() lager dummy dataframe fra kategorisk variabel. Kan spesifisere predix=\"string\" slik at de får kolonnenavn [\"string_0\",\"string_1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.idxmax()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
