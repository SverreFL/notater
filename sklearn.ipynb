{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Har brukervenlig design. Ulike type estimatorer (prediktor/transformers) har samme interface slik at jeg kan behandle de på samme måte og dytte de inn i meta-estimatorer som pipeline/gridsearch (som igjen har samme interface utad). Kan også lage custom estimators så lengde de oppfører seg på samme måte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Estimator(...):\n",
    "    def __init__(self, hyperparam, ...):\n",
    "\n",
    "    def fit(self, X, y):   # Fit/model the training data\n",
    "        ...                # given data X and targets y\n",
    "        return self\n",
    "    \n",
    "    if predictor:\n",
    "        def predict(self, X):  \n",
    "            ...                \n",
    "            return y_pred\n",
    "    \n",
    "    elif transformer:\n",
    "        def transform(self, X):\n",
    "            ...\n",
    "            return X_transformed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "est = Estimator().fit(X,y)\n",
    "\n",
    "est.predict(X_new)\n",
    "est.score(X_new) # accuracy for classification, R^2 for regression\n",
    "est.transform() # vet ikke... reduserer til features som er viktig? hmhm\n",
    "\n",
    "# Classification\n",
    "est.predict_proba(X_new)\n",
    "est.decision_function(X_new) # mål på usikkerhet som ikke er sannsynlighet ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Har lignende interface ... Litt usikker på om det er noe poeng med train test split på unsupervised.\n",
    "```python\n",
    "est.transform(X_new) # bruke parameter den har lært fra fit til å transformere..\n",
    "est.fit_transform(X) # hvis kjøre fit og transform på samme\n",
    "est.predict(X) # labels for cluster\n",
    "est.predict_proba(X) # sannsynlighet hvis soft boundary (eks. gmm)\n",
    "est.score(X) # noe likelihood greier\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan lage egne estimatorer som følger API. Mine custom class må inherite fra `BaseEstimator`. Har implementasjon av get_params og set_params slik at de fungerer i med GridSearch og Pipeline\n",
    "\n",
    "Har mange Mixin classer som jeg ikke skjønner hva er... Mixin er mer generelt begrep; ikke bare noe i sklearn.\n",
    "\n",
    "- RegressorMixin\n",
    "- TransformerMixin\n",
    "- mm..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lage custom estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inheriter fra TransformerMixin for å få fit_transform til å bli definert ut fra fit og transforn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eksempel som avgrenser til subset av k mest viktige features (som bestemt av feks. feature importance i random forest). Bruker dette til å initialisere objekt og har dessuten hyperparameter k. Kan få dette inn i gridsearch som del av pipeline uten at jeg trenger å finne feature importance i hvert steg siden kun varierer k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_importances, k):\n",
    "        self.feature_importances = feature_importances\n",
    "        self.k = k\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[:, self.feature_indices_]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skalering av data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mange av algoritmene bruker mål på avstand mellom observasjonene i inputrommet. De numeriske verdiene vi observerer i datasettet avhenger av måleenhet og forskjeller langs ulike kolonner er ikke nødvendigvis sammenlignbare. Algoritmene gir bedre resultat dersom vi forsøker å konvertere til en felles skala.\n",
    "\n",
    "For numeriske verdier bruker vi som oftest `StandardScaler` som trekker fra gjennomsnitt og deler på standardavvik for hver av kolonnene slik at de får fordelinger med $\\bar{x}=0$ og $std(x)=1$. \n",
    "\n",
    "Hvis verdiene er ikke-negative kan vi beholde en sparse representasjon ved å bruke `MinMaxScaler` som skalerer verdi inn i intervall [0,1]. Hvis 0-verdi er minste verdi i opprinnelig kolonne så vil de fortsatt være 0 etter transformasjon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformere verdi i kolonne\n",
    "\n",
    "1. `PowerTransformer`\n",
    "2. `OneHotEncoder`\n",
    "3. `PolynomialFeatures`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan partisjonere kontinuerlig inputmengde og lage dummy for indikatorer om gitt observasjon er i gitt bin. Litt usikker på hvordan jeg kombinerer med original kontinuerlig verdi slik at binsene kommer i tillegg i stedet for å erstatte..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "kb = KBinsDiscretizer(n_bins, # antall bins (hyperparameter)\n",
    "                      encode, # sparse som default, kan spesifisere 'onehot-dense'\n",
    "                      strategy # quantile (lik antall obs), uniform (lik avstand)\n",
    "                      ).fit(X)\n",
    "kb.bin_edges_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan lage polynom transformasjoner. Antallet variabler øker veldig raskt når vi øker antall grader fordi det øker antallet faktorer i kryssproduktene/interaksjonene. I praksis bruker vi neppe degree > 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "poly = PolynomialFeatures(degree).fit_transform(X) # lærer ikke parametre i fit()..\n",
    "poly.get_feature_names()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TfidfVectorizer`... Tar tekstdokument som input, lage dict med unike ord, lage vektor som angir antall ganger hvert av de unike ordene fra dokumnetene er i hver dokument. \n",
    "\n",
    "tror generelt modulen i hovedsak har verktøy for å jobbe med tekstdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenker dette er nyttig hvis jeg har et stort datasett med mange observerte variabler og det ikke er opplagt hvilke som er relevante for å predikere utfallet vi er interessert i.\n",
    "\n",
    "Kunne prøvd alle kombinasjoner av variabler, med det er $2^k$ som vokser raskt. Trenger bedre strategi for å finne et subset av variabler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmer kan ofte ikke håndtere missing data, men medfører tap av informasjon dersom vi kaster bort kolonner der vi ikke har observert verdi for alle. Løsningen er å *impute*/predikere verdi på det som mangler. \n",
    "\n",
    "Har i hovedsak to metoder:\n",
    "1. SimpleImpute som bruker verdier langs den gitte kolonnen\n",
    "2. KNNImpute som for hver observasjon bruker subset av observasjoner som har lignende verdier langs andre kolonner og observert på den kolonnen som mangler for gitte observasjonen.\n",
    "\n",
    "Tror det er litt rom for domain knowledge her. Det er også poeng at det kan være ulike kilder til missingness (sannsynlighet for at missing)\n",
    "1. Totally at random\n",
    "2. Avhengig av verdi på annen kolonne\n",
    "3. Avhengig av verdi på den gitte kolonne\n",
    "\n",
    "Kan gi biased resultat dersom vi ikke tar hensyn til at det ikke er totally random.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "X_imp = KNNImputer(n_neighbours).fit_transform(X)\n",
    "\n",
    "X_imp = SimpleImputer(strategy=\n",
    "                      'mean',\n",
    "                      'median',\n",
    "                      'most_frequent', # kan også brukes på ikke-numerisk\n",
    "                      'constant' # må spesfisere verdien\n",
    "                      ).fit_transform()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Har metrics som tar y_pred og y_true og gir score\n",
    "\n",
    "Har scores som tar estimator som argument. Bruker dette i `GridSearchCV` hvis jeg vil ha andre metric enn accuracy som kriterie for valg av beste hyperparameter. Har make_scorer som er najs wrapper funksjon som konverter metric til scorer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kategorisk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Har ulike metrics for binær klassifisering:\n",
    "1. Accuracy (andel predikert i riktig kategori)\n",
    "2. Precision (andel av predikert positive som faktisk er positiv)\n",
    "3. Recall/sensitivitet (andel av faktisk positive som predikert positiv)\n",
    "\n",
    "Kan tenke på det som corona-test. Presisjon ser på andelen som får positivt svar som faktisk er smittet. Recall ser på andelen av de som er smittet som får positivt svar. Det er en tradeoff her som avhenger av treshold i decision_function(). Får å øke sensitiviteten må predikere positive for flere av grensetilfellene, men da får vi flere falske positive og dermed lavere presisjon. Merk også at vi kan få arbitrær høy sensitivitet ved å predikere alle positiv.\n",
    "\n",
    "I praksis vil vi måtte gjøre en avveining, og den optimale balansen avhenger av kostnadene ved ulike typer feil. Vi kan bruke noen sammendragsmål\n",
    "\n",
    "1. F1-score\n",
    "2. Areal under ROC-curve.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\text{F} = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP,FP,TN,FN i binær.\n",
    "\n",
    "Ulike typer feil i multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "plot_confusion_matrix(est, X_test, y_test,\n",
    "                      normalize=, # [pred, true, all]. Bruker vel mest true, summer over rekke\n",
    "                      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precision-Sensitivity tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan undersøke hvordan det avhenger i treshold til output av predict_proba() eller decision_function(). Makismerer accuracy ved å plassere i kategori med høyest sannsynlighet, men ikke alltid accuracy er målet.\n",
    "\n",
    "```python\n",
    "precision, recall, treshold = precision_recall_curve(y_test, clf.decision_function(X_test))\n",
    "\n",
    "# plotte begge som funksjon av treshold\n",
    "plt.plot(treshold, precision[:-1]) # vet ikke hvorfor output ikke har samme shape ...\n",
    "plt.plot(treshold, recall[:-1])\n",
    "\n",
    "# plotte sensitivitet mot presisjon\n",
    "plt.plot(recall, precision)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "fpr, tpr, treshold = roc_curve(y_test, clf.decision_function(X_test))\n",
    "auc = roc_auc_score(y_test, clf.decision_function(X_test))\n",
    "\n",
    "plt.plot(fpr, tpr, label=f'auc: {auc}')\n",
    "plt.legend()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- confusion_matrix(y_true, y_pred), kan bruke som input i visualisering\n",
    "- classification_report()\n",
    "- precision_recall_curve()\n",
    "- roc_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Har hjelpefunksjoner for raske plot\n",
    "```python\n",
    "plot_precision_recall_curve(pipe,X_test, y_test) # presisjon vs sensitivitet (vil opp til høyre)\n",
    "plot_roc_curve(pipe,X_test, y_test) # false positive rate vs true positive rate (vil opp til venstre)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enkapsulere alt arbeid i ett python objekt...Vi har transformers og predictors. Bruker output fra transformers (imputation, skalering, mm) som input i predictor. Kan gjøre dette steg for steg, men det blir fort veldig rotete. Kan derfor i stedet lage pipeline gjennomfører transformeringene steg for steg. \n",
    "\n",
    "Også fordel at pre-processing blir del av grid i kryssvalidering i stedet for kun kjøre kryssvalidering i siste steget. Kan velge parametre i preproccessing + mindre skjevhet av at den har lært egenskaper fra andre folds i treningsdata fra før\n",
    "\n",
    "\n",
    "Kan bruke .predict(), .score() og sånn direkte på pipeline. Kjører da gjennom samme prossess tror jeg slik at vi får test data som tilsvarer... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def fit(self, X, y):\n",
    "    X_transformed = X\n",
    "    for name, estimator in self.steps[:-1]:\n",
    "        # iterate over all but the final step\n",
    "        # fit and transform the data\n",
    "        X_transformed = estimator.fit_transform(X_transformed, y)\n",
    "    # fit the last step\n",
    "    self.steps[-1][1].fit(X_transformed, y)\n",
    "    return self\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def predict(self, X):\n",
    "    X_transformed = X\n",
    "    for step in self.steps[:-1]:\n",
    "        # iterate over all but the final step\n",
    "        # transform the data\n",
    "        X_transformed = step[1].transform(X_transformed)\n",
    "    # predict using the last step\n",
    "    return self.steps[-1][1].predict(X_transformed)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Konstruksjon av pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan initialisere `Pipeline` objekt direkte eller bruke `make_pipeline` funksjon. Føler at den andre metoden er bedre siden jeg slipper å eksplisitt navngi stegenene; følger automatisk konvensjon med navn på class i lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "steps = [est1(), est2(),..]\n",
    "pipe = make_pipeline(*steps, # merk at må bruke instance (object) ikke class\n",
    "                     memory='cache_folder') # kan lagre transformer-objekt ... usikker på dette\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eksempel der vi bruker pipeline i grid search.\n",
    "```python\n",
    "steps = [StandardScaler(), PolynomialFeatures(), Ridge()]\n",
    "pipe = make_pipeline(*steps)\n",
    "param_grid={'polynomialfeatures__degree': [1, 2, 3],\n",
    "            'ridge__alpha': np.linspace(0.1, 2, 20)}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5).fit(X_train,y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tilgang til steg og deres attributter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan få ut predictor-objekt i siste steg med\n",
    "```python\n",
    "est = pipe.named_steps['navnpåestimator']\n",
    "```\n",
    "dette kan være praktisk siden den har metoder og attributter vil vi ha tak i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan sette hyperparametre til pipe med\n",
    "\n",
    "`navn_på_steg__navn_på_param`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ColumnTransformer`.. vil gjøre ulike transformasjoner på ulike subsets av kolonner. Kjører parallelt inne i pipeline. Merk at vi må bruke columntransformer først i pipeline fordi den tar DataFrame som input, mens andre estimatorer returner arrays fra .fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ct = make_column_transformer((OneHotEncoder(), cat_features), # første arg (transformer, list_of_cols)\n",
    "                             remainder=StandardScaler()) # hvilken transformasjon på resten.\n",
    "pipe = make_pipeline(ct, Ridge()) # kombinerer til pipe med prediktor i siste ledd. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I stedet for å lage en eksplisitt liste av kategoriske features kan jeg også bruke `make_column_selector` som konstruerer inndeling ut fra kriterie\n",
    "```python\n",
    "ct = make_column_transformer((OneHotEncoder(),\n",
    "                              make_column_selector(dtype_include=np.object)),\n",
    "                             (StandardScaler(),\n",
    "                              make_column_selector(dtype_include=np.number)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan finne cross validate score for gitt algoritme med gitt hyperparameter med \n",
    "\n",
    "```python\n",
    "cross_val_score(estimator, X, y, scoring) # splitter i 5 folds, trener på treningsdata og gir list med score på test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I praksis vil vi kjøre cv på grid av modeller. Kan gjøre det manuelt for en gitt 2-fold deling slik:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "k = range(1, 11)\n",
    "\n",
    "for n_neighbors in k:\n",
    "    # build the model\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X_train, y_train)\n",
    "    # record training and test set accuracy\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. validation_curve()\n",
    "2. learning_curve() # undersøke verdi av å legge til mer data (eventuelt se om aktuelt å øke fleksibilitet)\n",
    "\n",
    "```python\n",
    "param_grid = np.arange(1,10+1)\n",
    "train_scores, test_scores = validation_curve(KNeighborsClassifier(),X_train,y_train,'n_neighbors',param_grid)\n",
    "train_scores_mean = np.mean(train_scores,axis=1)\n",
    "test_scores_mean = np.mean(test_scores,axis=1) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV er en *meta-estimator* og har lignende api som vanlig estimator. Når vi caller .fit() gjør den kryssvalidering på grid internt, finner beste hyperparameter og trener deretter modellen på hele treningsdata. Det er denne estimatoren som deretter brukes når vi caller .predict() og .score().\n",
    "\n",
    "Vi kan undersøke resultatene fra kryssvalideringen med .cv_results_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "GridSearchCV(estimator, # i praksis bruker jeg Pipeline som (meta)estimator\n",
    "             param_grid, # dict, bruker {navn__param_navn:[vals], ..} til å angi hvilken est i pipe\n",
    "             scoring, # scorer, string (mapper til objekt) eller eksplisitt callable. Kan ta liste.\n",
    "                      # VIKTIG: forsøker å maxe score; dersom loss (eg. MSE) må jeg bruke negativ verdi. \n",
    "             return_train_score=True # default er False, kan sette True hvis jeg vil plotte..\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis jeg bare vil ha enkel train/validation split for hvert kombinasjon av hyperparameter (f.eks. hvis jeg har veldig mye data og/eller dårlig tid) kan jeg bruke\n",
    "\n",
    "```python\n",
    "single_split_cv = ShuffleSplit(n_splits=1)\n",
    "grid = GridSearchCV(estimator, param_grid, cv=single_split_cv)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearch og Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I praksis bruker vi `Pipeline` som estimator i `GridSearchCV`. Den beste estimatoren er dermed et Pipeline-objekt. Hvis vi vil undersøke egenskaper til prediktor i siste steg, for eksempel for å se på coeffisientene i logistisk regresjon, må vi bruke\n",
    "\n",
    "```python\n",
    "grid.best_estimator_.named_steps['logistic_regression'].coef_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualisere gridsearch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid.cv_results_ er en dictionary som fort blir veldig stor og uoversiktelig med stor grid. Kan få mye bedre representasjon hvis jeg transformerer det til dataframe. Kan lage plots (type validation curves og sånn) med utgangspunkt i dette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hvert punkt i grid har det info om\n",
    "1. Hvor lang tid det tar\n",
    "2. Parameterkombinasjon (dict)\n",
    "3. Verdi av hver parameter\n",
    "4. Score på hver fold\n",
    "5. Gjennomsnittlig score og std. avvik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pd.DataFrame(grid.cv_results_)[['param_ridge__alpha','mean_train_score','mean_test_score']]\n",
    "```\n",
    "Hvis en scorer er navn bare `score`, hvis flere scores så bruker navn på scorer i stedet, eks `mean_test_neg_mean_square_error`. \n",
    "\n",
    "Tror jeg har lyst til å lage hjelpe-funksjon som plotter resultat.. Litt vanskelig med flere parametre.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fordeler med `RandomizedSearchCV`:\n",
    "1. Slipper å spesifisere eksakt steps i grid (liker ikke arbritrære valg)\n",
    "2. Kontrollere kompleksitet med n_iter (i stedet for å tenke på størrelse av grid)\n",
    "3. Robust for irrelevant parameter (Randomisert så hver steg har variasjon i relevante, i stedet for å loope over fixed verdi av andre parametre for hver verdi av irrelevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "rs = RandomizedSearchSV(pipe,\n",
    "                        param_distributions, # dict med {'navn':fordeling,..} der fordeling har .rvs()\n",
    "                        n_iter # spesifisere hvor mange trekk fra fordelingene. Kontrollere lengde på search\n",
    "                        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utvide til grid search på flere predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kunne kjørt loop på ulik grid for ulik predictor, f.eks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "preprocess_steps = [ct, KNNImputer()]\n",
    "\n",
    "predictors = [KNeighborsClassifier(), LinearSVC(tol=1)]\n",
    "param_grids = [{'kneighborsclassifier__n_neighbors':np.arange(1,30+1,5)},\n",
    "               {'linearsvc__C':np.linspace(0.1,3,10)}\n",
    "              ]\n",
    "\n",
    "grids = []\n",
    "for i in range(len(predictors)):\n",
    "    steps = preprocess_steps+[predictors[i]]\n",
    "    pipe = make_pipeline(*steps)\n",
    "    grid = GridSearchCV(pipe,param_grids[i]).fit(X_train, y_train)\n",
    "    grids.append(grid)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dette er kanskje litt lite ryddig, og det blir litt ekstraarbeid siden den kjører preprocess steps på nytt... Kan prøve å få alt inn i et enkelt GridSearch objekt, men da blir det kanskje litt vanskelig å se på .cv_results_ siden det er ulik parametergrid for ulike predictors ...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kan også bruke param_grid som er liste med dictionary... én for hver pipe.. men alt inn i samme Pipeline objekt... hmhmhm\n",
    "\n",
    "```python \n",
    "pipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', SVC())])\n",
    "\n",
    "param_grid = [\n",
    "    {'classifier': [SVC()],\n",
    "     'preprocessing': [StandardScaler(), None],\n",
    "     'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "     'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "    {'classifier': [RandomForestClassifier(n_estimators=100)],\n",
    "     'preprocessing': [None],\n",
    "     'classifier__max_features': [1, 2, 3]}]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting og bagging.. bedre enn enkelt modell. Redusere varians eller redusere bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "rf = RandomForestClassifier(n_estimators).fit(X,y) # vet ikke behov for andre å tune andre parametre..\n",
    "\n",
    "feature_importance = rf.feature_importance_ # har noe med reduksjon i impurity for split på feature ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruker til å lage baseline modell. Sammenligningsgrunnlag for mer kompliserte modeller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "DummyClassifier(strategy=\n",
    "                'stratified', # tilfeldig ut fra andel i training se\n",
    "                'most_frequent', # alltid predikere majoritet\n",
    "                'uniform', # tilfeldig\n",
    "                'constant' # alltid predikere angitt verdi\n",
    "               )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "gmm = GaussianMixture(n_components, \n",
    "                      covariance_type # full, tied, diag, spherical.. spesifisere om de har komponent til felles\n",
    "                      )\n",
    "# Methods\n",
    "gmm.bic(X) # BIC på sample som fittet modellen på..\n",
    "\n",
    "# pdf i én dimensjona\n",
    "grid = np.linspace(-6,6,1000)\n",
    "logprob = gmm.score_samples(grid[:,None])\n",
    "pdf = np.exp(logprob)\n",
    "\n",
    "# pdf i to dimensjoner\n",
    "xx1, xx2 = np.mgrid[-1.5:2.5:0.01,-1:1.5:0.01]\n",
    "xx = np.array([xx1.ravel(), xx2.ravel()]).T\n",
    "pdf = np.exp(gmm.score_samples(xx)).reshape(xx1.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metoder for å gruppere observasjoner. Vil dele de inn i grupper slik at observasjonene er mest mulig like innad i grupper, samtidig som det er avstand mellom gruppene. Vi trenger da et mål på avstand mellom observasjoner. Et vanlig valg er eukledisk avstand mellom observasjonsvektorer, og det er da viktig at de har standardisert måleenhet slik at avstanden er meningsfull.\n",
    "\n",
    "Tror vi kan bruke det til å lære mer om fordelingen i populasjonen. Kan tenke oss at det er en uobservert variabel assosiert med hver observasjon som indikerer slags gruppetilhørighet som vi forsøker å gjette oss til fra realiserte data... Vi kan deretter bruke dette til å beskrive egenskaper til de betingede fordelingene til hver av de konstruerte gruppene i populasjonen vår."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Har ulike algoritmer som gjør ulike antagelser om gruppene i den datagenerende prosessen. Tror jeg i hovedssak ser på\n",
    "1. K-means: Minimerer avstand til center. Antar dermed at cluster er spredt i sirkel rundt hvert sentrum\n",
    "2. Gaussian mixture: Har i tillegg kovariansmatrise som lærer fra data, åpner for ulik form. Dessuten soft boundary; har estimat på sannsynlighet for at ny observasjon tilhører hver av gruppene\n",
    "3. DBSCAN (Density based spatital clustering with added noise): hm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "kmeans = KMeans(n_clusters).fit(X)\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.centers_\n",
    "\n",
    "# albuemetode for å bestemme antall clusters. Vet ikke om kan utvide til andre algoritmer. trenger i så fall en metric\n",
    "inertias = []\n",
    "k_grid = range(1,6+1)\n",
    "for k in k_grid:\n",
    "    inertias.append(KMeans(k).fit(X).inertia_)\n",
    "\n",
    "plt.plot(k_grid, inertias)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisere estimert sannsynlig for positiv klasse i binær klassifisering\n",
    "```python\n",
    "treshold = 0.5 # kan velge cut-off for decision boundary. default er 0.5\n",
    "x0_min, x0_max = min(X[:, 0])-.2, max(X[:, 0])+.2\n",
    "x1_min, x1_max = min(X[:, 1])-.2, max(X[:, 1])+.2\n",
    "xx0, xx1 = np.mgrid[x0_min:x0_max:.01, x1_min:x1_max:.01]\n",
    "\n",
    "clf = clf.fit(X,y)\n",
    "yy = clf.predict_proba(np.c_[xx0.ravel(), xx1.ravel()])[:,1] # sannsynlig for positiv klasse\n",
    "yy = yy.reshape(xx0.shape)\n",
    "\n",
    "ax.contourf(xx0, xx1, yy, cmap='RdBu_r')\n",
    "ax.contour(xx0,xx1,yy,levels=[treshold], linestyles='dashed', linewidths=3, colors='black')\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu_r')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prosjekt workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skriver ned midlertidig for å internalisere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Må ha veldefinert målsetning. Ha forståelse for hvordan maskinlæringsalgoritmen blir plassert i kontekst... hva er bruksområde. I praksis så er det ofte en del av en pipeline det output blir input i beslutningsprosses nedstrøms. Må skape verdi for nedstrømsbrukere. Utforming av prosjekt og valg av algoritme avhenger til dels av deres behov (høyest mulig presisjon eller transparens/tractability).\n",
    "\n",
    "Hvorvidt prosjektet skaper verdi avhenger av forskjell til baseline/status quo. Se i hvilken grad vi øker presisjon og/eller reduserer kostnader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi begynner med å lage et workspace for prosjektet (ser her bort fra samarbeid/ekstern repo). Her kommer command line inn i bildet. Vil lage en root folder (?) og initialisere repo for versjonskontroll. Vil også lage et isolert virtuelt miljø for prosjektet som definerer hvilke _dependencies_ (eksterne kodebaser) den har tilgang til og i hvilke versjon. Ved å isolere det fra det globale miljøet som min partikulære pc har på det gitte tidspunktet så unngår jeg problem med at andre (med andre miljø) ikke kan kjøre det, og unngår også bugs som følge av endringer i ekstern kodebase.\n",
    "\n",
    "Vil også organisere prosjektet på en god måte slik at andre kan samarbeide og replikere arbeidet mitt. Gode rutiner er viktig og dette må jeg vel utvikle etter hvert. Prosjektet er iterativ prosess med utforskning og prøving/feiling, men vil lagre ryddig versjon. Lagre kode i moduler og ha et main script som abstaherer fra implementasjonsdetaljer. Unngå notebooks til annet enn ren presentasjon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laste data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vil unngå å manuelt laste ned data på min lokale masking og laste inn fra path. Det er poeng at det skal være enkelt for andre å bruke det samme scriptet til å oppnå samme resultat på sin pc. \n",
    "\n",
    "I praksis ligger data i relasjonell database og jeg må gjøre litt sql ting... litt usikker på om jeg kan gjøre dette fra python script eller om det blir separat del av workflow.\n",
    "\n",
    "Alternativ kan jeg laste data fra nettet. Kan være litt utfordring med at store datafiler gjerne er komprimert, så må automatisere unzippingen fra python scriptet.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Må først bli litt kjent med data. Om mulig: liste kolonner, litt beskrivelse om hva de måler inkl målenhet og datatype. Vil ha oversikt over manglende observasjoner, univariate fordeling (sentraltendens, spredning, outliars) og litt om relasjon mellom variabler (korrelasjon som sammendragsmål).\n",
    "\n",
    "Bruker pandas og seaborn til dette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modellering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeg inkluderer feature engineering som del av modellering. Hvor mye algoritme kan lære avhenger av hvordan vi representerer data. Spesielt viktig hvis vi har relativt få observasjon; hvis uendelig data så oppdager den hva som er viktig og hva som er trash. Kan bruke domanin knowledge til å konstrure variabler som er informativt for output (feriedager viktig for reise mm.)\n",
    "\n",
    "Kan til dels bruke teori/domain knowledge til å bestemme hvilke variabler som er relevant eller ikke. Men dette kan til dels testes/automatiseres, så god idé å gjøre dette. Kan blant annet lage nye variabler som er kombinasjon av eksisterende (interaksjoner) mm. Vi operasjonaliserer dette gjennom transformers i sklearn med hyperparametre som vi kan søke over (eks: parameter som indikerer om vi inkluderer interaksjoner eller ikke).\n",
    "\n",
    "Må dessuten skalere før vi sender inn i algoritme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Presentasjon av prosjekt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hva oppnådde vi? Resultat i forhold til baseline. Hva kan vi lære fra modellen? Hvilke inputs er viktige, noe om retning på effekten... gir mer kredibilitet dersom vi forstår $h$ vi har estimert. Performance på ulike segment av testdata.. kan vi si noe om systematiske svakheter/styrke.. forstå hvorfor den fungerer såpass bra som den gjør og ikke bedre.\n",
    "\n",
    "Dernest kan vi si noe om prosessen. Hva som fungerte og hva som ikke gjorde det og noe om veien videre. Mulighet for bedre data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produksjon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lagrer modell med ... hva? Litt usikker. Plassere i pipeline slik at får data inn og spytter ut output. Kan være at den mottar queries fra webside der bruker plugger inn input, hva vet vel jeg... tror ikke dette kan være mitt ansvar. for da være grenser. \n",
    "\n",
    "Kan også være greit å overvåke input og output for å se at ting er kosher og at modellen oppfører seg som den skal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
